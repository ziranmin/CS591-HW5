{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from translate import Translator\n",
    "import numpy as np\n",
    "import preprocessor as p\n",
    "import re\n",
    "import codecs\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.stem.snowball import SpanishStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from translate import Translator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Original Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ziranmin/Desktop/Assignment5/train/english_train.text\", 'r') as f:\n",
    "    en_train_texts = [l.strip() for l in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/ziranmin/Desktop/Assignment5/train/english_train.labels', 'r') as f:\n",
    "    en_train_labels = [int(l.strip()) for l in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ziranmin/Desktop/Assignment5/train/spanish_train.text\", 'r') as f:\n",
    "    sp_train_texts = [l.strip() for l in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/ziranmin/Desktop/Assignment5/train/spanish_train.labels', 'r') as f:\n",
    "    sp_train_labels = [int(l.strip()) for l in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Original Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ziranmin/Desktop/Assignment5/test/english_test.text\", 'r') as f:\n",
    "    en_test_texts = [l.strip() for l in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ziranmin/Desktop/Assignment5/test/english_test.labels\", 'r') as f:\n",
    "    en_test_labels = [int(l.strip()) for l in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ziranmin/Desktop/Assignment5/test/spanish_test.text\", 'r') as f:\n",
    "    sp_test_texts = [l.strip() for l in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ziranmin/Desktop/Assignment5/test/spanish_test.labels\", 'r') as f:\n",
    "    sp_test_labels = [int(l.strip()) for l in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Mapping Labels¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ziranmin/Desktop/Assignment5/mapping/english_mapping.txt\", 'r') as f:\n",
    "    en_mapping = [l.strip() for l in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ziranmin/Desktop/Assignment5/mapping/spanish_mapping.txt\", 'r') as f:\n",
    "    sp_mapping = [l.strip() for l in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Labels that only appears in English or Spanish Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_remove_list = [4, 10, 12, 14, 15, 17, 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_remove_list = [6,  7,  8,  14,  16,  17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unmatched_label(text, label, remove_list):\n",
    "    all_index_to_remove = []\n",
    "    for i in remove_list:\n",
    "        train_has_one_of_unmatched_index =  [index for index, value in enumerate(label) if value == i]\n",
    "        all_index_to_remove += train_has_one_of_unmatched_index\n",
    "    all_index_to_remove.sort()\n",
    "    \n",
    "    texts_removed = [text[i] for i in range(len(text)) if i not  in all_index_to_remove]\n",
    "    labels_removed = [label[i] for i in range(len(label)) if i not  in all_index_to_remove]\n",
    "    \n",
    "    return texts_removed, labels_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_train_texts_removed,  en_train_labels_removed = remove_unmatched_label(en_train_texts, en_train_labels,  en_remove_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_test_texts_removed,  en_test_labels_removed = remove_unmatched_label(en_test_texts, en_test_labels,  en_remove_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_train_texts_removed,  sp_train_labels_removed = remove_unmatched_label(sp_train_texts, sp_train_labels, sp_remove_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_test_texts_removed,  sp_test_labels_removed = remove_unmatched_label(sp_test_texts, sp_test_labels, sp_remove_list) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rematch indices of Spanish tweets to corresponding indices of English tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rematched_sp_labels(sp_labels):\n",
    "    sp_labels_removed_rematched = []\n",
    "    sp_labels_removed_rematched = [ 8 if  i == 11 else i for i in sp_labels]\n",
    "    sp_labels_removed_rematched = [ 19 if  i == 13 else i for i in sp_labels_removed_rematched]\n",
    "    sp_labels_removed_rematched = [ 7 if  i == 15 else i for i in sp_labels_removed_rematched]\n",
    "    sp_labels_removed_rematched = [ 16 if  i == 18 else i for i in sp_labels_removed_rematched]\n",
    "    sp_labels_removed_rematched = [ 13 if  i == 12 else i for i in sp_labels_removed_rematched]\n",
    "    sp_labels_removed_rematched = [ 11 if  i == 9 else i for i in sp_labels_removed_rematched]\n",
    "    sp_labels_removed_rematched = [ 9 if  i == 5 else i for i in sp_labels_removed_rematched]\n",
    "    return sp_labels_removed_rematched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_train_labels_removed_rematched = rematched_sp_labels(sp_train_labels_removed)\n",
    "sp_test_labels_removed_rematched = rematched_sp_labels(sp_test_labels_removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning for Untranslated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.set_options(p.OPT.URL, p.OPT.SMILEY, p.OPT.MENTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = str.maketrans(\"\", \"\", punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stem_en = EnglishStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_train_texts_special_char = []\n",
    "for i in range(len(en_train_texts_removed)):\n",
    "    en_train_texts_special_char += re.sub(\"[A-Z]|[À-Ú]|[a-z]|[à-ú]|[0-9]|[' ']\", '', en_train_texts_removed[i])\n",
    "    \n",
    "sp_train_texts_special_char = []\n",
    "for i in range(len(sp_train_texts_removed)):\n",
    "    sp_train_texts_special_char += re.sub(\"[A-Z]|[À-Ú]|[a-z]|[à-ú]|[0-9]|[' ']\", '', sp_train_texts_removed[i])\n",
    "    \n",
    "en_test_texts_special_char = []\n",
    "for i in range(len(en_test_texts_removed)):\n",
    "    en_test_texts_special_char += re.sub(\"[A-Z]|[À-Ú]|[a-z]|[à-ú]|[0-9]|[' ']\", '', en_test_texts_removed[i])\n",
    "    \n",
    "sp_test_texts_special_char = []\n",
    "for i in range(len(sp_test_texts_removed)):\n",
    "    sp_test_texts_special_char += re.sub(\"[A-Z]|[À-Ú]|[a-z]|[à-ú]|[0-9]|[' ']\", '', sp_test_texts_removed[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_special_char = np.unique(en_train_texts_special_char + sp_train_texts_special_char + en_test_texts_special_char + sp_test_texts_special_char) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_special_char  = all_special_char.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_special_char += ['...']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_char_single_string = ''\n",
    "for i in range(len(all_special_char)):\n",
    "    special_char_single_string += all_special_char[i]\n",
    "    \n",
    "special_char_single_string = '[' + special_char_single_string + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_english(texts):\n",
    "    result = []\n",
    "    for text in texts:\n",
    "        #remove URL links, Smiley, and @user\n",
    "        text = p.clean(text)\n",
    "        #remove special characters\n",
    "        text = re.sub(special_char_single_string, '', text)\n",
    "        #make everything lower case\n",
    "        text = text.lower()\n",
    "        #remove stopwords\n",
    "        text = ' '.join([i for i in text.split() if i not in (stopwords.words('english'))])\n",
    "        #remove punctuation\n",
    "        text = text.translate(translator)\n",
    "        #change every word to stem word\n",
    "        text = [get_stem_en.stem(i) for i  in word_tokenize(text)]\n",
    "        result.append(' '.join(text))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_train_texts_removed_cleaned = clean_english(en_train_texts_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_test_texts_removed_cleaned = clean_english(en_test_texts_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open(\"en_train_texts_removed_cleaned.txt\",'w',\"utf-8\") as out_fs:\n",
    "    for each in en_train_texts_removed_cleaned:\n",
    "        out_fs.write(each + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open(\"en_test_texts_removed_cleaned.txt\",'w',\"utf-8\") as out_fs:\n",
    "    for each in en_test_texts_removed_cleaned:\n",
    "        out_fs.write(each + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stem_sp = SpanishStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_spanish(texts):\n",
    "    result = []\n",
    "    \n",
    "    for text in texts:\n",
    "        #remove URL links, Smiley, and @user\n",
    "        text = p.clean(text)\n",
    "        #remove special characters\n",
    "        text = re.sub(special_char_single_string, '', text)\n",
    "        #make everything lower case\n",
    "        text = text.lower()\n",
    "        #remove stopwords\n",
    "        text = ' '.join([i for i in text.split() if i not in (stopwords.words('spanish'))])\n",
    "        #remove punctuation\n",
    "        text = text.translate(translator)\n",
    "        #change every word to stem word\n",
    "        text = [get_stem_sp.stem(i) for i  in word_tokenize(text)]\n",
    "        result.append(' '.join(text))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_train_texts_removed_cleaned = clean_spanish(sp_train_texts_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_test_texts_removed_cleaned = clean_spanish(sp_test_texts_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open(\"sp_train_texts_removed_cleaned.txt\",'w',\"utf-8\") as out_fs:\n",
    "    for each in sp_train_texts_removed_cleaned:\n",
    "        out_fs.write(each + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open(\"sp_test_texts_removed_cleaned.txt\",'w',\"utf-8\") as out_fs:\n",
    "    for each in sp_test_texts_removed_cleaned:\n",
    "        out_fs.write(each + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Line Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ziranmin/Desktop/Assignment5/en_train_texts_removed_cleaned.txt\", 'r') as f:\n",
    "    en_train_texts_removed_cleaned_read = [l.strip() for l in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ziranmin/Desktop/Assignment5/en_test_texts_removed_cleaned.txt\", 'r') as f:\n",
    "    en_test_texts_removed_cleaned_read = [l.strip() for l in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_train_tf = tf.fit_transform(en_train_texts_removed_cleaned_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_test_tf = tf.transform(en_test_texts_removed_cleaned_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F-Score (official): 22.19\n",
      "-----\n",
      "Micro F-Score: 36.884\n",
      "Precision: 36.884\n",
      "Recall: 36.884\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LR_en = LogisticRegression(C=4)\n",
    "LR_en.fit(en_train_tf, en_train_labels_removed)\n",
    "    \n",
    "en_pred = LR_en.predict(en_test_tf)\n",
    "    \n",
    "np.savetxt('predicted_labels_file.txt', en_pred, fmt='%d')\n",
    "np.savetxt('gold_labels_file.txt', np.array(en_test_labels_removed), fmt='%d')\n",
    "%run scorer_semeval18.py gold_labels_file.txt predicted_labels_file.txt\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spanis to English Translation and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = str.maketrans(\"\", \"\", punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_to_en_translator =  Translator(from_lang=\"spanish\", to_lang=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sp_to_en(texts):\n",
    "    result = []\n",
    "\n",
    "    for txt in texts:\n",
    "         #remove URL links, Smiley, and @user\n",
    "        txt = p.clean(txt)\n",
    "        #remove special characters\n",
    "        txt = re.sub(special_char_single_string, '', txt)\n",
    "        #remove punctuation\n",
    "        txt = txt.translate(trans)\n",
    "        \n",
    "        #Spanish to English translation\n",
    "        txt = sp_to_en_translator.translate(txt)\n",
    "        #make everything lower case\n",
    "        txt = txt.lower()\n",
    "        #remove stopwords\n",
    "        txt = ' '.join([i for i in txt.split() if i not in (stopwords.words('english'))])\n",
    "        #change every word to stem word\n",
    "        txt = [get_stem_en.stem(i) for i  in word_tokenize(txt)]\n",
    "        \n",
    "        result.append(' '.join(txt))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_train_translated = sp_to_en(sp_train_texts_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_test_translated = sp_to_en(sp_test_texts_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_en_train_text = en_train_texts_removed_cleaned_read + en_train_translated\n",
    "new_en_test_text = en_test_texts_removed_cleaned_read + en_test_translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_en_train_labels = en_train_labels_removed + sp_train_labels_removed\n",
    "new_en_test_labels = en_test_labels_removed + sp_test_labels_removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Modeling¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer()\n",
    "new_en_train_tf = tf.fit_transform(new_en_train_text)\n",
    "new_en_test_tf = tf.transform(new_en_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F-Score (official): 15.643\n",
      "-----\n",
      "Micro F-Score: 30.78\n",
      "Precision: 30.78\n",
      "Recall: 30.78\n"
     ]
    }
   ],
   "source": [
    "LR_new = LogisticRegression(C=4)\n",
    "LR_new.fit(new_en_train_tf, new_en_train_labels)\n",
    "    \n",
    "en_pred = LR_new.predict(new_en_test_labels)\n",
    "    \n",
    "np.savetxt('predicted_labels_file.txt', en_pred, fmt='%d')\n",
    "np.savetxt('gold_labels_file.txt', np.array(new_en_test_labels), fmt='%d')\n",
    "%run scorer_semeval18.py gold_labels_file.txt predicted_labels_file.txt\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
